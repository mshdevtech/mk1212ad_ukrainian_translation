#!/usr/bin/env python3
"""
merge_tsv.py
‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
–°–∫—Ä–∏–ø—Ç –¥–ª—è –º–µ—Ä–¥–∂—É (–æ–Ω–æ–≤–ª–µ–Ω–Ω—è) —Ñ–∞–π–ª—ñ–≤ –ø–µ—Ä–µ–∫–ª–∞–¥—É –∑ –æ—Ä–∏–≥—ñ–Ω–∞–ª–æ–º

–©–æ —Ä–æ–±–∏—Ç—å:
  - –î–æ–¥–∞—î –Ω–æ–≤—ñ –∫–ª—é—á—ñ –∑ –æ—Ä–∏–≥—ñ–Ω–∞–ª—É (EN) —É –≤—ñ–¥–ø–æ–≤—ñ–¥–Ω—ñ —Ñ–∞–π–ª–∏ –ø–µ—Ä–µ–∫–ª–∞–¥—É
  - –ù–µ –∑–∞—Ç–∏—Ä–∞—î –≤–∂–µ –ø–µ—Ä–µ–∫–ª–∞–¥–µ–Ω—ñ —Ä—è–¥–∫–∏
  - –ê—Ä—Ö—ñ–≤—É—î –≤–∏–¥–∞–ª–µ–Ω—ñ –∫–ª—é—á—ñ —É –ø–∞–ø–∫—É obsolete
  - –í–∞–ª—ñ–¥—É—î —Å—Ç—Ä—É–∫—Ç—É—Ä—É —Ç–∞ —É–Ω—ñ–∫–∞–ª—å–Ω—ñ—Å—Ç—å –∫–ª—é—á—ñ–≤ —É TSV

–Ø–∫ –∑–∞–ø—É—Å–∫–∞—Ç–∏:
  python scripts/merge_tsv.py

–î–ª—è —á–æ–≥–æ –ø–æ—Ç—Ä—ñ–±–Ω–æ:
  - –©–æ–± –ø–µ—Ä–µ–∫–ª–∞–¥ –∑–∞–≤–∂–¥–∏ –º—ñ—Å—Ç–∏–≤ —É—Å—ñ –∞–∫—Ç—É–∞–ª—å–Ω—ñ –∫–ª—é—á—ñ –∑ –æ—Ä–∏–≥—ñ–Ω–∞–ª—É
  - –©–æ–± –Ω–µ –≤—Ç—Ä–∞—á–∞—Ç–∏ –≤–∂–µ –∑—Ä–æ–±–ª–µ–Ω–∏–π –ø–µ—Ä–µ–∫–ª–∞–¥
  - –î–ª—è –∑—Ä—É—á–Ω–æ–≥–æ –æ–Ω–æ–≤–ª–µ–Ω–Ω—è –ø—ñ—Å–ª—è –æ–Ω–æ–≤–ª–µ–Ω–Ω—è –æ—Ä–∏–≥—ñ–Ω–∞–ª—å–Ω–∏—Ö —Ñ–∞–π–ª—ñ–≤
"""

import pandas as pd, pathlib, sys
import csv

SRC_DIR = pathlib.Path("_upstream/en/text/db")
TRG_DIR = pathlib.Path("translation/text/db")
OBS_DIR = pathlib.Path("obsolete")

# - stat counters -
files_done   = 0
total_added  = 0
total_removed = 0
total_modified = 0

# ‚îÄ‚îÄ –§—É–Ω–∫—Ü—ñ—ó –≤–∞–ª—ñ–¥–∞—Ü—ñ—ó ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
def validate_tsv_file(file_path: pathlib.Path) -> tuple[bool, list[str]]:
    """–í–∞–ª—ñ–¥—É—î –æ–¥–∏–Ω TSV —Ñ–∞–π–ª —Ç–∞ –ø–æ–≤–µ—Ä—Ç–∞—î (is_valid, error_messages)."""
    errors = []
    try:
        df = pd.read_csv(file_path, sep="\t", dtype=str, keep_default_na=False, quoting=csv.QUOTE_NONE, encoding_errors='ignore')
    except Exception as e:
        errors.append(f"–Ω–µ –≤–¥–∞–ª–æ—Å—è –ø—Ä–æ—á–∏—Ç–∞—Ç–∏ —Ñ–∞–π–ª ({e})")
        return False, errors

    # –ü–µ—Ä–µ–≤—ñ—Ä—è—î–º–æ –∫–æ–ª–æ–Ω–∫–∏
    required_cols = ["key", "text", "tooltip"]
    if list(df.columns) != required_cols:
        errors.append(f"–æ—á—ñ–∫—É–≤–∞–Ω–æ –∫–æ–ª–æ–Ω–∫–∏ {required_cols}, –∞ –æ—Ç—Ä–∏–º–∞–Ω–æ {list(df.columns)}")
        return False, errors

    # –ü–æ—Ä–æ–∂–Ω—ñ key
    empty_rows = df["key"].str.strip() == ""
    if empty_rows.any():
        rows = ", ".join(map(str, (df.index[empty_rows] + 2)))  # +2: header + 0-based
        errors.append(f"–ø–æ—Ä–æ–∂–Ω—ñ–π key —É —Ä—è–¥–∫–∞—Ö {rows}")

    # –î—É–±–ª—ñ–∫–∞—Ç–∏ key
    non_empty_keys = df.loc[~empty_rows, "key"]
    dup_keys = non_empty_keys[non_empty_keys.duplicated()]
    if not dup_keys.empty:
        keys = ", ".join(dup_keys.unique())
        errors.append(f"–¥—É–±–ª—ñ–∫–∞—Ç–∏ key: {keys}")

    return len(errors) == 0, errors

def validate_directory(dir_path: pathlib.Path, dir_name: str) -> tuple[bool, dict[str, list[str]]]:
    """–í–∞–ª—ñ–¥—É—î –≤—Å—ñ TSV —Ñ–∞–π–ª–∏ –≤ –¥–∏—Ä–µ–∫—Ç–æ—Ä—ñ—ó —Ç–∞ –ø–æ–≤–µ—Ä—Ç–∞—î (is_valid, file_errors)."""
    print(f"üîç –ü–µ—Ä–µ–≤—ñ—Ä—è—î–º–æ TSV —É {dir_name} ({dir_path})...")
    
    if not dir_path.exists():
        print(f"‚ö†Ô∏è  –î–∏—Ä–µ–∫—Ç–æ—Ä—ñ—è {dir_name} –Ω–µ —ñ—Å–Ω—É—î")
        return True, {}
    
    file_errors = {}
    has_errors = False
    
    for file_path in sorted(dir_path.glob("*.loc.tsv")):
        is_valid, errors = validate_tsv_file(file_path)
        if errors:
            file_errors[file_path.name] = errors
            has_errors = True
            print(f"‚ùå {file_path.name}:")
            for error in errors:
                print(f"   ‚Ä¢ {error}")
        else:
            pass
    
    print()
    return not has_errors, file_errors

def ask_continue() -> bool:
    """–ü–∏—Ç–∞—î –∫–æ—Ä–∏—Å—Ç—É–≤–∞—á–∞ —á–∏ –ø—Ä–æ–¥–æ–≤–∂—É–≤–∞—Ç–∏ –≤–∏–∫–æ–Ω–∞–Ω–Ω—è."""
    while True:
        response = input("–ü—Ä–æ–¥–æ–≤–∂–∏—Ç–∏ –º–µ—Ä–¥–∂? (y/n): ").lower().strip()
        if response in ['y', 'yes', '—Ç–∞–∫', '—Ç']:
            return True
        elif response in ['n', 'no', '–Ω—ñ', '–Ω']:
            return False
        else:
            print("–ë—É–¥—å –ª–∞—Å–∫–∞, –≤–≤–µ–¥—ñ—Ç—å 'y' –∞–±–æ 'n'")

# ‚îÄ‚îÄ –ü–µ—Ä–µ–≤—ñ—Ä–∫–∞ —Ñ–∞–π–ª—ñ–≤ –ø–µ—Ä–µ–¥ –º–µ—Ä–¥–∂–µ–º ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
print("=== –ü–û–ü–ï–†–ï–î–ù–Ø –ü–ï–†–ï–í–Ü–†–ö–ê –§–ê–ô–õ–Ü–í ===\n")

src_valid, src_errors = validate_directory(SRC_DIR, "SRC_DIR")
trg_valid, trg_errors = validate_directory(TRG_DIR, "TRG_DIR")

if not src_valid or not trg_valid:
    print("‚ö†Ô∏è  –ó–ù–ê–ô–î–ï–ù–û –ü–û–ú–ò–õ–ö–ò –í –§–ê–ô–õ–ê–•!")
    print("–°–∫—Ä–∏–ø—Ç –º–æ–∂–µ –≤—ñ–¥–ø—Ä–∞—Ü—é–≤–∞—Ç–∏ –Ω–µ–∫–æ—Ä–µ–∫—Ç–Ω–æ —ñ –∫—Ä–∞—â–µ –≤–∏–ø—Ä–∞–≤–∏—Ç–∏ –ø—Ä–æ–±–ª–µ–º–Ω—ñ —Ñ–∞–π–ª–∏ –≤–ª–∞—Å–Ω–æ—Ä—É—á.")
    print()
    
    if not ask_continue():
        print("–ú–µ—Ä–¥–∂ —Å–∫–∞—Å–æ–≤–∞–Ω–æ.")
        sys.exit(1)
    
    print("–ü—Ä–æ–¥–æ–≤–∂—É—î–º–æ –º–µ—Ä–¥–∂...\n")
else:
    print("‚úÖ –í—Å—ñ —Ñ–∞–π–ª–∏ –≤–∞–ª—ñ–¥–Ω—ñ, –ø—Ä–æ–¥–æ–≤–∂—É—î–º–æ –º–µ—Ä–¥–∂.\n")

print("=== –ü–û–ß–ò–ù–ê–Ñ–ú–û –ú–ï–†–î–ñ ===\n")

files_with_changes = 0

for src_path in SRC_DIR.glob("*.loc.tsv"):
    trg_path = TRG_DIR / src_path.name
    read = lambda p: pd.read_csv(
        p, sep="\t", dtype=str, keep_default_na=False, na_filter=False, 
        quoting=csv.QUOTE_NONE, encoding_errors='ignore'
    )

    src = read(src_path)
    trg = read(trg_path) if trg_path.exists() else pd.DataFrame(columns=src.columns)

    # - Filter empty keys -
    src = src[src["key"].str.strip() != ""].copy()
    trg = trg[trg["key"].str.strip() != ""].copy()

    # - Merging -
    merged = src.merge(trg, on="key", how="left", suffixes=("", "_old"))

    # 1) if translation already exist ‚Äî keep it
    # 2) if translation doesn't exist ‚Äî copy original text
    merged["text"] = merged["text_old"].where(
        merged["text_old"].notna() & (merged["text_old"] != ""),        # —è–∫—â–æ text_old –ù–ï NaN ‚Üí –∑–∞–ª–∏—à–∞—î–º–æ
        merged["text"]                     # —ñ–Ω–∞–∫—à–µ –±–µ—Ä–µ–º–æ –∑–Ω–∞—á–µ–Ω–Ω—è –∑ text
    )

    # - Count actually modified rows (where translation appeared or changed) -
    modified_count = 0
    if "text_old" in merged.columns:
        modified_mask = (merged["text"].notna()) & (merged["text"] != "") & (
            merged["text_old"].isna() | (merged["text_old"] == "") | (merged["text"] != merged["text_old"]))
        modified_count = modified_mask.sum()
    total_modified += modified_count

    merged = merged[src.columns]   # return column order

    # NaN ‚Üí ""
    merged = merged.fillna("")

    trg_path.parent.mkdir(parents=True, exist_ok=True)
    
    # –ó–±–µ—Ä—ñ–≥–∞—î–º–æ –∑ pandas to_csv, –≤–∏–∫–æ—Ä–∏—Å—Ç–æ–≤—É—é—á–∏ QUOTE_NONE
    merged.to_csv(trg_path, sep='\t', index=False, quoting=csv.QUOTE_NONE, encoding='utf-8')

    # - statistic for new keys -
    new_keys = src.loc[~src["key"].isin(trg["key"])]
    total_added += len(new_keys)

    # -Ô∏é Removed keys -
    removed = trg.loc[~trg["key"].isin(src["key"])]
    if not removed.empty:
        OBS_DIR.mkdir(parents=True, exist_ok=True)
        
        # –ó–±–µ—Ä—ñ–≥–∞—î–º–æ –∞—Ä—Ö—ñ–≤–Ω–∏–π —Ñ–∞–π–ª –∑ pandas to_csv
        archive_path = OBS_DIR / src_path.name
        removed.to_csv(archive_path, sep='\t', index=False, quoting=csv.QUOTE_NONE, encoding='utf-8')
        
        total_removed += len(removed)

    files_done += 1
    if len(new_keys) > 0 or len(removed) > 0 or modified_count > 0:
        print(f"‚úì {src_path.name}: +{len(new_keys)} new, -{len(removed)} removed, ~{modified_count} modified")
        files_with_changes += 1

if files_with_changes == 0:
    print("‚úÖ –í—Å—ñ —Ñ–∞–π–ª–∏ –∞–∫—Ç—É–∞–ª—å–Ω—ñ")

print("\n=== Merge completed ===")
print(f"Processed files : {files_done}")
print(f"New keys added  : {total_added}")
print(f"Keys archived   : {total_removed}")
print(f"Rows modified   : {total_modified}")
print("Done!")

sys.exit(0)