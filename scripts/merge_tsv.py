# This script merges translation files from a source directory (`_upstream/en/text/db`)
# with corresponding files in a target directory (`translation/text/db`).
# It performs the following steps:
#
# 1. Reads all `.loc.tsv` files from the source directory.
# 2. For each source file:
#    - Reads the corresponding target file if it exists, or creates an empty DataFrame.
#    - Filters out rows with empty or whitespace-only keys.
#    - Merges the source and target files on the `key` column:
#      - Keeps existing translations from the target file if they are non-empty.
#      - Copies the source text if no translation exists in the target file.
#    - Ensures the column order matches the source file and replaces NaN values with empty strings.
#    - Saves the merged file back to the target directory.
# 3. Tracks and logs statistics:
#    - Counts new keys added from the source file to the target file.
#    - Identifies and archives keys removed from the source file into an `obsolete` directory.
# 4. Outputs a summary of processed files, new keys added, and keys archived.
#
# Usage:
# - Place the source `.loc.tsv` files in `_upstream/en/text/db`.
# - Place the target `.loc.tsv` files (if any) in `translation/text/db`.
# - Run the script. The merged files will be saved in `translation/text/db`,
#   and removed keys will be archived in the `obsolete` directory.
import pandas as pd, pathlib, sys
import csv

SRC_DIR = pathlib.Path("_upstream/en/text/db")
TRG_DIR = pathlib.Path("translation/text/db")
OBS_DIR = pathlib.Path("obsolete")

# - stat counters -
files_done   = 0
total_added  = 0
total_removed = 0
total_modified = 0

# â”€â”€ Ð¤ÑƒÐ½ÐºÑ†Ñ–Ñ— Ð²Ð°Ð»Ñ–Ð´Ð°Ñ†Ñ–Ñ— â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
def validate_tsv_file(file_path: pathlib.Path) -> tuple[bool, list[str]]:
    """Ð’Ð°Ð»Ñ–Ð´ÑƒÑ” Ð¾Ð´Ð¸Ð½ TSV Ñ„Ð°Ð¹Ð» Ñ‚Ð° Ð¿Ð¾Ð²ÐµÑ€Ñ‚Ð°Ñ” (is_valid, error_messages)."""
    errors = []
    try:
        df = pd.read_csv(file_path, sep="\t", dtype=str, keep_default_na=False, quoting=csv.QUOTE_NONE, encoding_errors='ignore')
    except Exception as e:
        errors.append(f"Ð½Ðµ Ð²Ð´Ð°Ð»Ð¾ÑÑ Ð¿Ñ€Ð¾Ñ‡Ð¸Ñ‚Ð°Ñ‚Ð¸ Ñ„Ð°Ð¹Ð» ({e})")
        return False, errors

    # ÐŸÐµÑ€ÐµÐ²Ñ–Ñ€ÑÑ”Ð¼Ð¾ ÐºÐ¾Ð»Ð¾Ð½ÐºÐ¸
    required_cols = ["key", "text", "tooltip"]
    if list(df.columns) != required_cols:
        errors.append(f"Ð¾Ñ‡Ñ–ÐºÑƒÐ²Ð°Ð½Ð¾ ÐºÐ¾Ð»Ð¾Ð½ÐºÐ¸ {required_cols}, Ð° Ð¾Ñ‚Ñ€Ð¸Ð¼Ð°Ð½Ð¾ {list(df.columns)}")
        return False, errors

    # ÐŸÐ¾Ñ€Ð¾Ð¶Ð½Ñ– key
    empty_rows = df["key"].str.strip() == ""
    if empty_rows.any():
        rows = ", ".join(map(str, (df.index[empty_rows] + 2)))  # +2: header + 0-based
        errors.append(f"Ð¿Ð¾Ñ€Ð¾Ð¶Ð½Ñ–Ð¹ key Ñƒ Ñ€ÑÐ´ÐºÐ°Ñ… {rows}")

    # Ð”ÑƒÐ±Ð»Ñ–ÐºÐ°Ñ‚Ð¸ key
    non_empty_keys = df.loc[~empty_rows, "key"]
    dup_keys = non_empty_keys[non_empty_keys.duplicated()]
    if not dup_keys.empty:
        keys = ", ".join(dup_keys.unique())
        errors.append(f"Ð´ÑƒÐ±Ð»Ñ–ÐºÐ°Ñ‚Ð¸ key: {keys}")

    return len(errors) == 0, errors

def validate_directory(dir_path: pathlib.Path, dir_name: str) -> tuple[bool, dict[str, list[str]]]:
    """Ð’Ð°Ð»Ñ–Ð´ÑƒÑ” Ð²ÑÑ– TSV Ñ„Ð°Ð¹Ð»Ð¸ Ð² Ð´Ð¸Ñ€ÐµÐºÑ‚Ð¾Ñ€Ñ–Ñ— Ñ‚Ð° Ð¿Ð¾Ð²ÐµÑ€Ñ‚Ð°Ñ” (is_valid, file_errors)."""
    print(f"ðŸ” ÐŸÐµÑ€ÐµÐ²Ñ–Ñ€ÑÑ”Ð¼Ð¾ TSV Ñƒ {dir_name} ({dir_path})...")
    
    if not dir_path.exists():
        print(f"âš ï¸  Ð”Ð¸Ñ€ÐµÐºÑ‚Ð¾Ñ€Ñ–Ñ {dir_name} Ð½Ðµ Ñ–ÑÐ½ÑƒÑ”")
        return True, {}
    
    file_errors = {}
    has_errors = False
    
    for file_path in sorted(dir_path.glob("*.loc.tsv")):
        is_valid, errors = validate_tsv_file(file_path)
        if errors:
            file_errors[file_path.name] = errors
            has_errors = True
            print(f"âŒ {file_path.name}:")
            for error in errors:
                print(f"   â€¢ {error}")
        else:
            print(f"âœ… {file_path.name}")
    
    print()
    return not has_errors, file_errors

def ask_continue() -> bool:
    """ÐŸÐ¸Ñ‚Ð°Ñ” ÐºÐ¾Ñ€Ð¸ÑÑ‚ÑƒÐ²Ð°Ñ‡Ð° Ñ‡Ð¸ Ð¿Ñ€Ð¾Ð´Ð¾Ð²Ð¶ÑƒÐ²Ð°Ñ‚Ð¸ Ð²Ð¸ÐºÐ¾Ð½Ð°Ð½Ð½Ñ."""
    while True:
        response = input("ÐŸÑ€Ð¾Ð´Ð¾Ð²Ð¶Ð¸Ñ‚Ð¸ Ð¼ÐµÑ€Ð´Ð¶? (y/n): ").lower().strip()
        if response in ['y', 'yes', 'Ñ‚Ð°Ðº', 'Ñ‚']:
            return True
        elif response in ['n', 'no', 'Ð½Ñ–', 'Ð½']:
            return False
        else:
            print("Ð‘ÑƒÐ´ÑŒ Ð»Ð°ÑÐºÐ°, Ð²Ð²ÐµÐ´Ñ–Ñ‚ÑŒ 'y' Ð°Ð±Ð¾ 'n'")

# â”€â”€ ÐŸÐµÑ€ÐµÐ²Ñ–Ñ€ÐºÐ° Ñ„Ð°Ð¹Ð»Ñ–Ð² Ð¿ÐµÑ€ÐµÐ´ Ð¼ÐµÑ€Ð´Ð¶ÐµÐ¼ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
print("=== ÐŸÐ Ð•Ð”Ð’ÐÐ Ð˜Ð¢Ð•Ð›Ð¬ÐÐ ÐŸÐ•Ð Ð•Ð’Ð†Ð ÐšÐ Ð¤ÐÐ™Ð›Ð†Ð’ ===\n")

src_valid, src_errors = validate_directory(SRC_DIR, "SRC_DIR")
trg_valid, trg_errors = validate_directory(TRG_DIR, "TRG_DIR")

if not src_valid or not trg_valid:
    print("âš ï¸  Ð—ÐÐÐ™Ð”Ð•ÐÐž ÐŸÐžÐœÐ˜Ð›ÐšÐ˜ Ð’ Ð¤ÐÐ™Ð›ÐÐ¥!")
    print("Ð¡ÐºÑ€Ð¸Ð¿Ñ‚ Ð¼Ð¾Ð¶Ðµ Ð²Ñ–Ð´Ð¿Ñ€Ð°Ñ†ÑŽÐ²Ð°Ñ‚Ð¸ Ð½ÐµÐºÐ¾Ñ€ÐµÐºÑ‚Ð½Ð¾ Ñ– ÐºÑ€Ð°Ñ‰Ðµ Ð²Ð¸Ð¿Ñ€Ð°Ð²Ð¸Ñ‚Ð¸ Ð¿Ñ€Ð¾Ð±Ð»ÐµÐ¼Ð½Ñ– Ñ„Ð°Ð¹Ð»Ð¸ Ð²Ð»Ð°ÑÐ½Ð¾Ñ€ÑƒÑ‡.")
    print()
    
    if not ask_continue():
        print("ÐœÐµÑ€Ð´Ð¶ ÑÐºÐ°ÑÐ¾Ð²Ð°Ð½Ð¾.")
        sys.exit(1)
    
    print("ÐŸÑ€Ð¾Ð´Ð¾Ð²Ð¶ÑƒÑ”Ð¼Ð¾ Ð¼ÐµÑ€Ð´Ð¶...\n")
else:
    print("âœ… Ð’ÑÑ– Ñ„Ð°Ð¹Ð»Ð¸ Ð²Ð°Ð»Ñ–Ð´Ð½Ñ–, Ð¿Ñ€Ð¾Ð´Ð¾Ð²Ð¶ÑƒÑ”Ð¼Ð¾ Ð¼ÐµÑ€Ð´Ð¶.\n")

print("=== ÐŸÐžÐ§Ð˜ÐÐÐ„ÐœÐž ÐœÐ•Ð Ð”Ð– ===\n")

for src_path in SRC_DIR.glob("*.loc.tsv"):
    trg_path = TRG_DIR / src_path.name
    read = lambda p: pd.read_csv(
        p, sep="\t", dtype=str, keep_default_na=False, na_filter=False, 
        quoting=csv.QUOTE_NONE, encoding_errors='ignore'
    )

    src = read(src_path)
    trg = read(trg_path) if trg_path.exists() else pd.DataFrame(columns=src.columns)

    # - Filter empty keys -
    src = src[src["key"].str.strip() != ""].copy()
    trg = trg[trg["key"].str.strip() != ""].copy()

    # - Merging -
    merged = src.merge(trg, on="key", how="left", suffixes=("", "_old"))

    # 1) if translation already exist â€” keep it
    # 2) if translation doesn't exist â€” copy original text
    merged["text"] = merged["text_old"].where(
        merged["text_old"].notna() & (merged["text_old"] != ""),        # ÑÐºÑ‰Ð¾ text_old ÐÐ• NaN â†’ Ð·Ð°Ð»Ð¸ÑˆÐ°Ñ”Ð¼Ð¾
        merged["text"]                     # Ñ–Ð½Ð°ÐºÑˆÐµ Ð±ÐµÑ€ÐµÐ¼Ð¾ Ð·Ð½Ð°Ñ‡ÐµÐ½Ð½Ñ Ð· text
    )

    # - Count actually modified rows (where translation appeared or changed) -
    modified_count = 0
    if "text_old" in merged.columns:
        modified_mask = (merged["text"].notna()) & (merged["text"] != "") & (
            merged["text_old"].isna() | (merged["text_old"] == "") | (merged["text"] != merged["text_old"]))
        modified_count = modified_mask.sum()
    total_modified += modified_count

    merged = merged[src.columns]   # return column order

    # NaN â†’ ""
    merged = merged.fillna("")

    trg_path.parent.mkdir(parents=True, exist_ok=True)
    
    # Ð—Ð±ÐµÑ€Ñ–Ð³Ð°Ñ”Ð¼Ð¾ Ð· pandas to_csv, Ð²Ð¸ÐºÐ¾Ñ€Ð¸ÑÑ‚Ð¾Ð²ÑƒÑŽÑ‡Ð¸ QUOTE_NONE
    merged.to_csv(trg_path, sep='\t', index=False, quoting=csv.QUOTE_NONE, encoding='utf-8')

    # - statistic for new keys -
    new_keys = src.loc[~src["key"].isin(trg["key"])]
    total_added += len(new_keys)

    # -ï¸Ž Removed keys -
    removed = trg.loc[~trg["key"].isin(src["key"])]
    if not removed.empty:
        OBS_DIR.mkdir(parents=True, exist_ok=True)
        
        # Ð—Ð±ÐµÑ€Ñ–Ð³Ð°Ñ”Ð¼Ð¾ Ð°Ñ€Ñ…Ñ–Ð²Ð½Ð¸Ð¹ Ñ„Ð°Ð¹Ð» Ð· pandas to_csv
        archive_path = OBS_DIR / src_path.name
        removed.to_csv(archive_path, sep='\t', index=False, quoting=csv.QUOTE_NONE, encoding='utf-8')
        
        total_removed += len(removed)

    files_done += 1
    if len(new_keys) > 0 or len(removed) > 0 or modified_count > 0:
        print(f"âœ“ {src_path.name}: +{len(new_keys)} new, -{len(removed)} removed, ~{modified_count} modified")

print("\n=== Merge completed ===")
print(f"Processed files : {files_done}")
print(f"New keys added  : {total_added}")
print(f"Keys archived   : {total_removed}")
print(f"Rows modified   : {total_modified}")
print("Done!")

sys.exit(0)